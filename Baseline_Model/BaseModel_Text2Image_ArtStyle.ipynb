{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYsJH_0g0BFb"
      },
      "outputs": [],
      "source": [
        "# Run this line in Colab to install the package if it is\n",
        "# not already installed.\n",
        "!pip install git+https://github.com/openai/glide-text2im\n",
        "\n",
        "# Get the Library containing the Art Style\n",
        "!git clone https://github.com/PaddlePaddle/PaddleGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2ngBULNRH09",
        "outputId": "6b1db5e7-c928-47e4-8a31-1f6739228954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect the google drive repo\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiR9d69tTaNC"
      },
      "outputs": [],
      "source": [
        "# Import the Coco Dataset\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Set paths for the coco dataset\n",
        "dataDir='/content/drive/MyDrive/COCOdataset2017Airplanes'\n",
        "dataType='train2017'\n",
        "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF6tM40v0BFd"
      },
      "outputs": [],
      "source": [
        "# Different imports used by the Text to Image model\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch as th\n",
        "import cv2\n",
        "\n",
        "from glide_text2im.download import load_checkpoint\n",
        "from glide_text2im.model_creation import (\n",
        "    create_model_and_diffusion,\n",
        "    model_and_diffusion_defaults,\n",
        "    model_and_diffusion_defaults_upsampler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_aazCXJ0BFd"
      },
      "outputs": [],
      "source": [
        "# This notebook supports both CPU and GPU.\n",
        "# On CPU, generating one sample may take on the order of 20 minutes.\n",
        "# On a GPU, it should be under a minute.\n",
        "has_cuda = th.cuda.is_available()\n",
        "device = th.device('cpu' if not has_cuda else 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vjvUHJ70BFe"
      },
      "outputs": [],
      "source": [
        "# Create base model.\n",
        "options = model_and_diffusion_defaults()\n",
        "options['use_fp16'] = has_cuda\n",
        "options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n",
        "model, diffusion = create_model_and_diffusion(**options)\n",
        "model.eval()\n",
        "if has_cuda:\n",
        "    model.convert_to_fp16()\n",
        "model.to(device)\n",
        "model.load_state_dict(load_checkpoint('base', device))\n",
        "print('total base parameters', sum(x.numel() for x in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMcuZHsQ0BFe"
      },
      "outputs": [],
      "source": [
        "# Create upsampler model.\n",
        "options_up = model_and_diffusion_defaults_upsampler()\n",
        "options_up['use_fp16'] = has_cuda\n",
        "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
        "model_up.eval()\n",
        "if has_cuda:\n",
        "    model_up.convert_to_fp16()\n",
        "model_up.to(device)\n",
        "model_up.load_state_dict(load_checkpoint('upsample', device))\n",
        "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqqDv6qz0BFe"
      },
      "outputs": [],
      "source": [
        "def show_images(batch: th.Tensor):\n",
        "    \"\"\" Display a batch of images inline. \"\"\"\n",
        "    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "    display(Image.fromarray(reshaped.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoWc0tGE5g3W"
      },
      "outputs": [],
      "source": [
        "def save_image(batch: th.Tensor, i):\n",
        "  \"\"\" Save an image into the colab space \"\"\"\n",
        "  scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "  reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "  \n",
        "  img = Image.fromarray(reshaped.numpy())\n",
        "  path = \"/content/Images/output_text2img_\" + str(i) + \".png\"\n",
        "  img.save(path,\"PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDSfQXrmTt-w"
      },
      "outputs": [],
      "source": [
        "# Initialize the COCO api for instance annotations\n",
        "coco=COCO(annFile)\n",
        "\n",
        "# Load the categories in a variable\n",
        "catIDs = coco.getCatIds()\n",
        "cats = coco.loadCats(catIDs)\n",
        "\n",
        "# Define the classes (out of the 81) which you want to see. Others will not be shown.\n",
        "filterClasses = ['airplane']\n",
        "\n",
        "# Fetch class IDs only corresponding to the filterClasses\n",
        "catIds = coco.getCatIds(catNms=filterClasses) \n",
        "# Get all images containing the above Category IDs\n",
        "imgIds = coco.getImgIds(catIds=catIds)\n",
        "\n",
        "# initialize COCO API for caption captions\n",
        "captions_annFile = '{}/annotations/captions_{}_planes.json'.format(dataDir,dataType)\n",
        "coco_caps = COCO(captions_annFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkAopgbEUsTo"
      },
      "outputs": [],
      "source": [
        "# Lists to contain the images info\n",
        "images_annot = []\n",
        "images_paths = []\n",
        "images_styles = []\n",
        "\n",
        "# Run on the Test set part\n",
        "for i in range(2586, 2985):\n",
        "  # Training set from 0 to 1999\n",
        "  # Validation set from 2000 to 2585\n",
        "  # Test set from 2586 to 2985\n",
        "\n",
        "  # Get annotations and their positions\n",
        "  annIds = coco_caps.getAnnIds(imgIds=imgIds[i])\n",
        "  anns = coco_caps.loadAnns(annIds)\n",
        "\n",
        "  # Split the annotations to have the art style and the caption separated\n",
        "  splitted = anns[0]['caption'].split(\" as \")\n",
        "  \n",
        "  # Keep the caption\n",
        "  images_annot.append(splitted[0])\n",
        "  images_styles.append(splitted[1].split(\".\")[0])\n",
        "\n",
        "  #Get file paths\n",
        "  img=coco.loadImgs(imgIds[i])[0]\n",
        "  content_img_path='{}/New_Art_Airplanes_Final/{}'.format(dataDir,img['file_name'])\n",
        "\n",
        "  # Keep the path\n",
        "  images_paths.append(content_img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFmxSFuW0BFf"
      },
      "outputs": [],
      "source": [
        "# Run over 400 test images\n",
        "for i in range(0, 400):\n",
        "  # Sampling parameters\n",
        "  prompt = images_annot[i]\n",
        "  batch_size = 1\n",
        "  guidance_scale = 3.0\n",
        "\n",
        "  # Tune this parameter to control the sharpness of 256x256 images.\n",
        "  # A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
        "  upsample_temp = 0.997\n",
        "\n",
        "  ##############################\n",
        "  # Sample from the base model #\n",
        "  ##############################\n",
        "\n",
        "  # Create the text tokens to feed to the model.\n",
        "  tokens = model.tokenizer.encode(prompt)\n",
        "  tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
        "      tokens, options['text_ctx']\n",
        "  )\n",
        "\n",
        "  # Create the classifier-free guidance tokens (empty)\n",
        "  full_batch_size = batch_size * 2\n",
        "  uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
        "      [], options['text_ctx']\n",
        "  )\n",
        "\n",
        "  # Pack the tokens together into model kwargs.\n",
        "  model_kwargs = dict(\n",
        "      tokens=th.tensor(\n",
        "          [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
        "      ),\n",
        "      mask=th.tensor(\n",
        "          [mask] * batch_size + [uncond_mask] * batch_size,\n",
        "          dtype=th.bool,\n",
        "          device=device,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  # Create a classifier-free guidance sampling function\n",
        "  def model_fn(x_t, ts, **kwargs):\n",
        "     half = x_t[: len(x_t) // 2]\n",
        "     combined = th.cat([half, half], dim=0)\n",
        "     model_out = model(combined, ts, **kwargs)\n",
        "     eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "     cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "     half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "     eps = th.cat([half_eps, half_eps], dim=0)\n",
        "     return th.cat([eps, rest], dim=1)\n",
        "\n",
        "  # Sample from the base model.\n",
        "  model.del_cache()\n",
        "  samples = diffusion.p_sample_loop(\n",
        "      model_fn,\n",
        "      (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
        "      device=device,\n",
        "      clip_denoised=True,\n",
        "      progress=True,\n",
        "      model_kwargs=model_kwargs,\n",
        "      cond_fn=None,\n",
        "  )[:batch_size]\n",
        "  model.del_cache()\n",
        "\n",
        "  ##############################\n",
        "  # Upsample the 64x64 samples #\n",
        "  ##############################\n",
        "\n",
        "  tokens = model_up.tokenizer.encode(prompt)\n",
        "  tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
        "      tokens, options_up['text_ctx']\n",
        "  )\n",
        "\n",
        "  # Create the model conditioning dict.\n",
        "  model_kwargs = dict(\n",
        "      # Low-res image to upsample.\n",
        "      low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "      # Text tokens\n",
        "      tokens=th.tensor(\n",
        "          [tokens] * batch_size, device=device\n",
        "      ),\n",
        "      mask=th.tensor(\n",
        "          [mask] * batch_size,\n",
        "          dtype=th.bool,\n",
        "          device=device,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  # Sample from the base model.\n",
        "  model_up.del_cache()\n",
        "  up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
        "  up_samples = diffusion_up.ddim_sample_loop(\n",
        "      model_up,\n",
        "      up_shape,\n",
        "      noise=th.randn(up_shape, device=device) * upsample_temp,\n",
        "      device=device,\n",
        "      clip_denoised=True,\n",
        "      progress=True,\n",
        "      model_kwargs=model_kwargs,\n",
        "      cond_fn=None,\n",
        "  )[:batch_size]\n",
        "  model_up.del_cache()\n",
        "\n",
        "  # Save & show the output\n",
        "  # Show_images(up_samples)\n",
        "  save_image(up_samples, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5QngK3ZfiJF"
      },
      "outputs": [],
      "source": [
        "# Start with Paddle, begin the Art Style\n",
        "%cd PaddleGAN/\n",
        "\n",
        "# Install required packages\n",
        "!pip install -r requirements.txt\n",
        "!pip install paddlepaddle\n",
        "\n",
        "import requests\n",
        "\n",
        "# Initialize the distances variables\n",
        "distanceTotal = 0\n",
        "distances_list = []\n",
        "\n",
        "for i in range(0,400):\n",
        "  # Paths\n",
        "  PATH_OF_CONTENT_IMG = '/content/Images/output_text2img_' + str(i) + '.png'\n",
        "  PATH_OF_STYLE_IMG = '/content/Style_Images/' + images_styles[i] + '.png'\n",
        "  OUTPUT_PATH = '/content/Output/'\n",
        "  STYLE = images_styles[i]\n",
        "\n",
        "  # Use the lapstyle model from the PaddleGan library\n",
        "  !python applications/tools/lapstyle.py --content_img_path {PATH_OF_CONTENT_IMG} --style_image_path {PATH_OF_STYLE_IMG} --style {STYLE} --output_path {OUTPUT_PATH}\n",
        "\n",
        "  # Use the DeepAI API to get similarity between two images\n",
        "  r = requests.post(\n",
        "      \"https://api.deepai.org/api/image-similarity\",\n",
        "      files={\n",
        "          'image1': open('/content/Images/output_text2img_' + str(i) + '.png', 'rb'),\n",
        "          'image2': open('/content/Output/LapStyle/stylized.png', 'rb'),\n",
        "      },\n",
        "      headers={'api-key': '##API-KEY##'}\n",
        "  )\n",
        "  # Get and store the different distances\n",
        "  distance = r.json()['output']['distance']\n",
        "  distanceTotal += distance\n",
        "  distances_list.append(distance)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugbqDe6iH-tg"
      },
      "outputs": [],
      "source": [
        "# transform the distance list to a DataFrame using Pandas\n",
        "# to export as a CSV\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(distances_list)\n",
        "df.to_csv('distances.csv')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text2Im2ArtStyle.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e7d6e62d90e7e85f9a0faa7f0b1d576302d7ae6108e9fe361594f8e1c8b05781"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
