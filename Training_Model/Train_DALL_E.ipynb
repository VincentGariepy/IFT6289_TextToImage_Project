{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_DALL-E",
      "provenance": [],
      "authorship_tag": "ABX9TyPcBdujEPh+FeiKIMi+C7N6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentGariepy/IFT6289_TextToImage_Project/blob/main/Training_Model/Train_DALL_E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht56doNPLFEK"
      },
      "outputs": [],
      "source": [
        "!pip install dalle-pytorch --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "dh_R8L7GLI48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "uzqgsURkLI9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip TrainingImages_Airplanes.zip\n",
        "!rm -rf TrainingImages_Airplanes.zip"
      ],
      "metadata": {
        "id": "4AlPlJXeLaly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import choice\n",
        "from pathlib import Path\n",
        "\n",
        "# torch\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# vision imports\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# dalle related classes and utils\n",
        "\n",
        "from dalle_pytorch import OpenAIDiscreteVAE, DiscreteVAE, DALLE\n",
        "from dalle_pytorch.tokenizer import tokenizer, HugTokenizer\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "# argument parsing\n",
        "\n",
        "VAE_PATH = None   # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
        "DALLE_PATH = None # './dalle.pt'\n",
        "TAMING = False  # use VAE from taming transformers paper\n",
        "IMAGE_TEXT_FOLDER = './TrainingImages_Airplanes'\n",
        "BPE_PATH = None\n",
        "RESUME = exists(DALLE_PATH)\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "GRAD_CLIP_NORM = 0.5\n",
        "\n",
        "MODEL_DIM = 512\n",
        "TEXT_SEQ_LEN = 256\n",
        "DEPTH = 2\n",
        "HEADS = 4\n",
        "DIM_HEAD = 64\n",
        "REVERSIBLE = True\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "# tokenizer\n",
        "\n",
        "if BPE_PATH is not None:\n",
        "    tokenizer = HugTokenizer(BPE_PATH)\n",
        "\n",
        "# reconstitute vae\n",
        "\n",
        "if RESUME:\n",
        "    dalle_path = Path(DALLE_PATH)\n",
        "    assert dalle_path.exists(), 'DALL-E model file does not exist'\n",
        "\n",
        "    loaded_obj = torch.load(str(dalle_path))\n",
        "\n",
        "    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
        "\n",
        "    if vae_params is not None:\n",
        "        vae = DiscreteVAE(**vae_params)\n",
        "    else:\n",
        "        vae = OpenAIDiscreteVAE()\n",
        "\n",
        "    dalle_params = dict(        \n",
        "        **dalle_params\n",
        "    )\n",
        "\n",
        "    IMAGE_SIZE = vae.image_size\n",
        "\n",
        "else:\n",
        "    if exists(VAE_PATH):\n",
        "        vae_path = Path(VAE_PATH)\n",
        "        assert vae_path.exists(), 'VAE model file does not exist'\n",
        "\n",
        "        loaded_obj = torch.load(str(vae_path))\n",
        "\n",
        "        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n",
        "\n",
        "        vae = DiscreteVAE(**vae_params)\n",
        "        vae.load_state_dict(weights)\n",
        "    else:\n",
        "        print('using pretrained VAE for encoding images to tokens')\n",
        "        vae_params = None\n",
        "\n",
        "        vae_klass = OpenAIDiscreteVAE if not TAMING else VQGanVAE1024\n",
        "        vae = vae_klass()\n",
        "\n",
        "    IMAGE_SIZE = vae.image_size\n",
        "\n",
        "    dalle_params = dict(\n",
        "        num_text_tokens = VOCAB_SIZE,\n",
        "        text_seq_len = TEXT_SEQ_LEN,\n",
        "        dim = MODEL_DIM,\n",
        "        depth = DEPTH,\n",
        "        heads = HEADS,\n",
        "        dim_head = DIM_HEAD,\n",
        "        reversible = REVERSIBLE\n",
        "    )\n",
        "\n",
        "# helpers\n",
        "\n",
        "def save_model(path):\n",
        "    save_obj = {\n",
        "        'hparams': dalle_params,\n",
        "        'vae_params': vae_params,\n",
        "        'weights': dalle.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(save_obj, path)\n",
        "\n",
        "# dataset loading\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, folder, text_len = 256, image_size = 128):\n",
        "        super().__init__()\n",
        "        path = Path(folder)\n",
        "\n",
        "        text_files = [*path.glob('**/*.txt')]\n",
        "\n",
        "        image_files = [\n",
        "            *path.glob('**/*.png'),\n",
        "            *path.glob('**/*.jpg'),\n",
        "            *path.glob('**/*.jpeg')\n",
        "        ]\n",
        "\n",
        "        text_files = {t.stem: t for t in text_files}\n",
        "        image_files = {i.stem: i for i in image_files}\n",
        "\n",
        "        keys = (image_files.keys() & text_files.keys())\n",
        "\n",
        "        self.keys = list(keys)\n",
        "        self.text_files = {k: v for k, v in text_files.items() if k in keys}\n",
        "        self.image_files = {k: v for k, v in image_files.items() if k in keys}\n",
        "\n",
        "        self.image_tranform = T.Compose([\n",
        "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "            T.RandomResizedCrop(image_size, scale = (0.75, 1.), ratio = (1., 1.)),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        key = self.keys[ind]\n",
        "        text_file = self.text_files[key]\n",
        "        image_file = self.image_files[key]\n",
        "\n",
        "        image = Image.open(image_file)\n",
        "        descriptions = text_file.read_text().split('\\n')\n",
        "        descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
        "        description = choice(descriptions)\n",
        "\n",
        "        tokenized_text = tokenizer.tokenize(description).squeeze(0)\n",
        "        mask = tokenized_text != 0\n",
        "\n",
        "        image_tensor = self.image_tranform(image)\n",
        "        return tokenized_text, image_tensor, mask\n",
        "\n",
        "# create dataset and dataloader\n",
        "\n",
        "ds = TextImageDataset(\n",
        "    IMAGE_TEXT_FOLDER,\n",
        "    text_len = TEXT_SEQ_LEN,\n",
        "    image_size = IMAGE_SIZE\n",
        ")\n",
        "\n",
        "assert len(ds) > 0, 'dataset is empty'\n",
        "print(f'{len(ds)} image-text pairs found for training')\n",
        "\n",
        "dl = DataLoader(ds, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
        "\n",
        "# initialize DALL-E\n",
        "\n",
        "dalle = DALLE(vae = vae, **dalle_params).cuda()\n",
        "\n",
        "if RESUME:\n",
        "    dalle.load_state_dict(weights)\n",
        "\n",
        "# optimizer\n",
        "\n",
        "opt = Adam(dalle.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "# experiment tracker\n",
        "\n",
        "import wandb\n",
        "\n",
        "model_config = dict(\n",
        "    depth = DEPTH,\n",
        "    heads = HEADS,\n",
        "    dim_head = DIM_HEAD\n",
        ")\n",
        "\n",
        "run = wandb.init(project = 'dalle_train_transformer', resume = RESUME, config = model_config)\n",
        "\n",
        "# training\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (text, images, mask) in enumerate(dl):\n",
        "        text, images, mask = map(lambda t: t.cuda(), (text, images, mask))\n",
        "\n",
        "        loss = dalle(text, images, return_loss = True)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(dalle.parameters(), GRAD_CLIP_NORM)\n",
        "\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        log = {}\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(epoch, i, f'loss - {loss.item()}')\n",
        "\n",
        "            log = {\n",
        "                **log,\n",
        "                'epoch': epoch,\n",
        "                'iter': i,\n",
        "                'loss': loss.item()\n",
        "            }\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            sample_text = text[:1]\n",
        "            token_list = sample_text.masked_select(sample_text != 0).tolist()\n",
        "            decoded_text = tokenizer.decode(token_list)\n",
        "\n",
        "            image = dalle.generate_images(\n",
        "                text[:1],\n",
        "                filter_thres = 0.9    # topk sampling at 0.9\n",
        "            )\n",
        "\n",
        "            save_model(f'./dalle.pt')\n",
        "            wandb.save(f'./dalle.pt')\n",
        "\n",
        "            log = {\n",
        "                **log,\n",
        "                'image': wandb.Image(image, caption = decoded_text)\n",
        "            }\n",
        "\n",
        "        wandb.log(log)\n",
        "\n",
        "    # save trained model to wandb as an artifact every epoch's end\n",
        "\n",
        "    model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
        "    model_artifact.add_file('dalle.pt')\n",
        "    run.log_artifact(model_artifact)\n",
        "\n",
        "save_model(f'./dalle-final.pt')\n",
        "wandb.save('./dalle-final.pt')\n",
        "model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
        "model_artifact.add_file('dalle-final.pt')\n",
        "run.log_artifact(model_artifact)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "trDhAWWJMbmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}